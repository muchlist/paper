[{"content":"In Golang application development, we often find a single struct object used for various purposes, such as representing data in the database as well as payload in API requests and responses. Although this seems practical, this approach can actually create problems related to security and maintenance. This article will discuss the importance of separating DTO, Entity and Model by applying some Domain-Driven Design (DDD) principles.\nUnderstanding Entity, Model and DTO in Domain-Driven Design Principles Domain-Driven Design (DDD) is a software development methodology that focuses on separation of responsibilities through modeling oriented towards business domains. In DDD, we recognize several important concepts:\nData Transfer Object (DTO): Used to transfer data between functions without involving complex business logic. For example, structs for requests, responses, and function parameters. Entity: Used to store data that will be used in application logic. A struct is called an entity if it has an identity (such as an ID) that distinguishes it from other data. Entities can have their own logic. For example, a Weather entity that has an IsOutdoorEventFeasible() method to evaluate whether the weather is suitable for outdoor events. type WeatherEntity struct { ID string // example: Combination of Location Code and Timestamp City string Temperature float64 Humidity int Description string } // IsOutdoorEventFeasible evaluates whether the weather is suitable for outdoor events. func (w *WeatherEntity) IsOutdoorEventFeasible() bool { // outdoor events are considered not feasible if: // - Temperature below 15 degrees Celsius or above 35 degrees Celsius // - Weather description indicates rain or storm if w.Temperature \u0026lt; 15 || w.Temperature \u0026gt; 35 { return false } if w.Description == \u0026#34;rain\u0026#34; || w.Description == \u0026#34;storm\u0026#34; { return false } return true } Repository: Repository objects hide data storage implementation details. While Model structs function as data representation in the database used by Repository. Application Service: Handles business logic that requires interaction with external components or other services, in clean architecture this is often called usecase or service. Handles operations that don\u0026rsquo;t naturally fit within the context of Entity or Value Object. Actually there are many others, such as Value Object, Aggregate, Domain-Service etc. However, we want our code to be \u0026ldquo;good-enough for maintainability\u0026rdquo;, but also \u0026ldquo;not become too complex\u0026rdquo;, so here we are a bit loose in applying DDD.\nWhy is Separation Important? Using the same struct across different application layers like database, business logic, and presentation can create high coupling. For example, changes in the database (such as adding new columns) can affect the API, even if those columns are not relevant for API users.\nScenario Suppose we have an application that helps users plan events based on weather forecasts. Our application uses a third-party weather API to get current weather information.\ntype Weather struct { City string `json:\u0026#34;city\u0026#34; db:\u0026#34;city\u0026#34;` Temperature float64 `json:\u0026#34;temperature\u0026#34; db:\u0026#34;temperature\u0026#34;` Humidity int `json:\u0026#34;humidity\u0026#34; db:\u0026#34;humidity\u0026#34;` WindSpeed float64 `json:\u0026#34;wind_speed\u0026#34; db:\u0026#34;wind_speed\u0026#34;` Description string `json:\u0026#34;description\u0026#34; db:\u0026#34;description\u0026#34;` } One day, the third-party weather API announces changes to their response, adding more details like airQualityIndex, visibility, and uvIndex. They even make major changes to version 2 such as splitting temperature into temperature_celsius and temperature_kelvin.\nImpact Without Struct Separation (bad) If we use the same Weather struct to capture responses from the API, store data in the database, and also as our API response, changes in the third-party API can cause several problems:\nChanges in Many Places: Changes in one struct means also changing the database, business logic, and possibly also data consumed by the frontend. Overfetching and Irrelevant Data: we might not need all the additional data like temperature_kelvin or uvIndex for our application\u0026rsquo;s purposes, but because we use the same structure, we are forced to handle this extra data. Increased Complexity: With new data, we might need some modifications to the data types to adjust Tags, Marshalers, Scanners and Valuers. Impact With Struct Separation (good) Conversely, by separating DTO, Entity, and Model, we can more efficiently handle these changes.\nDTO (Data Transfer Object):\nWe create a dedicated struct to capture responses from the weather API that includes all new data (or only relevant data). Helps us to know data availability from the API.\nFor the above scenario, we only need to adjust the API Client layer.\ntype WeatherAPIResponse struct { City string `json:\u0026#34;city\u0026#34;` TemperatureCelsius float64 `json:\u0026#34;temperature_celsius\u0026#34;` TemperatureKelvin float64 `json:\u0026#34;temperature_kelvin\u0026#34;` Humidity int `json:\u0026#34;humidity\u0026#34;` WindSpeed float64 `json:\u0026#34;wind_speed\u0026#34;` Description string `json:\u0026#34;description\u0026#34;` AirQualityIndex int `json:\u0026#34;airQualityIndex\u0026#34;` Visibility int `json:\u0026#34;visibility\u0026#34;` UvIndex int `json:\u0026#34;uvIndex\u0026#34;` } func (w *WeatherAPIResponse) ToEntity(){ // transform } Entity:\nThe Weather entity in our application only stores data relevant to the application\u0026rsquo;s function, such as Temperature, Humidity, and Description. No need to store uvIndex or visibility if that data is not used in the event planning process, so we know which data is important for logic and which is not.\ntype WeatherEntity struct { ID string // Combination of Location Code and Timestamp City string Temperature float64 Humidity int Description string } // IsOutdoorEventFeasible evaluates whether the weather is suitable for outdoor events. func (w *WeatherEntity) IsOutdoorEventFeasible() bool { // outdoor events are considered not feasible if: // - Temperature below 15 degrees Celsius or above 35 degrees Celsius // - Weather description indicates rain or storm if w.Temperature \u0026lt; 15 || w.Temperature \u0026gt; 35 { return false } if w.Description == \u0026#34;rain\u0026#34; || w.Description == \u0026#34;storm\u0026#34; { return false } return true } Business Logic (Usecase Layer):\nBusiness logic should not know about database models or responses from third-party APIs. Business logic only processes data that is already in Entity form or that we can control its stability. This facilitates maintenance and reduces the risk of errors.\nDatabase Model:\nFor database storage purposes, use a separate struct, especially if using ORM\ntype WeatherModel struct { ID string `db:\u0026#34;id\u0026#34;` City string `db:\u0026#34;city\u0026#34;` Temperature float64 `db:\u0026#34;temperature\u0026#34;` Humidity int `db:\u0026#34;humidity\u0026#34;` Description string `db:\u0026#34;description\u0026#34;` } func (w *WeatherModel) ToEntity(){ // transform } func FromEntity(WeatherEntity) WeatherModel { // transform } and so on for WeatherRequestDTO and WeatherResponseDTO.\nTrade-offs Although separating data structures like DTO (Data Transfer Object), Entity, and database Model has long-term benefits such as security, ease in testing, and clear separation of concerns, there are some drawbacks that need to be considered as well. One of the main drawbacks is the need to perform transformations between these structs, which means there is a slight sacrifice in speed.\nHowever, this approach is often considered a reasonable price for the benefits gained. Popular books like Clean Code by Robert C. Martin, The Pragmatic Programmer by Andrew Hunt and David Thomas, and Refactoring: Improving the Design of Existing Code by Martin Fowler, often emphasize the importance of prioritizing correct and maintainable code before focusing on speed.\nBesides, the latency generated from this data transformation is very very very minimal compared to the latency of database operations, which tend to be a more significant bottleneck in many applications.\nWhen Should You NOT Separate Structs? The system is too simple. Requires high speed such as in game development. The slightest performance improvement is considered more important than readability and ease of maintenance. How to Properly Separate Structs I recommend the following approach to separate golang structs in API architecture. This approach ensures that each layer in the application has clear and separate responsibilities, making maintenance and future development easier.\nStructs for Presentation Layer: WeatherRequest and WeatherResponse: These structs are used to handle data coming in and out of the API (presentation). They are responsible for validating and formatting data according to client needs. For more complex cases, such as partial update features, you might need WeatherUpdateRequest. This version uses pointer fields to allow partial updates. Structs for Domain Layer: WeatherEntity: This entity represents data in the business domain and contains logic directly related to business rules. Entities should be stable and not affected by changes in other layers, such as databases or external APIs. For more complex cases, such as partial update features, you might need WeatherUpdateDTO. A DTO version that also uses pointer fields for flexibility in data transmission. Structs for Persistence Layer: WeatherModel: This struct is used for database interaction. This model reflects the storage schema and can change along with changes in the database layer. Implementation Diagram Assuming using Clean Architecture or Hexagonal Architecture, then:\nHandler Layer manages request and response data, converts requests to internal data types that we can fully control (entity) before passing to Usecase. Usecase Layer works with stable entities, this layer should avoid direct dependencies on database models or external API formats. Repository Layer manages database access and converts data to and from entities used by usecase. This approach ensures that each layer is isolated from irrelevant changes in other layers, thereby improving application resilience and flexibility. By separating responsibilities in each layer, applications become more modular, facilitating maintenance and scalability.\nAlso read: How to apply good rules to maintain separation of concerns\nConclusion Implementing separation of DTO, Entity and Model structs in API design using Golang is a small investment that can save a lot of time and resources for development and maintenance in the future, making our system not only efficient but also easy to manage and develop. This approach can clearly divide the responsibilities of each component, reduce dependencies between modules, and ultimately strengthen the overall application architecture itself.\nOf course, there is no one perfect approach for every situation. How has your experience been in implementing or perhaps not implementing this principle? Are there specific cases where you found more effective alternatives? Share your experience in the comments section!\n","permalink":"https://blog.muchlis.dev/en/post/struct-separation/","summary":"\u003cp\u003eIn Golang application development, we often find a single struct object used for various purposes, such as representing data in the database as well as payload in API requests and responses. Although this seems practical, this approach can actually create problems related to security and maintenance. This article will discuss the importance of separating DTO, Entity and Model by applying some Domain-Driven Design (DDD) principles.\u003c/p\u003e","title":"Understanding the Importance of Separating DTO, Entity and Model in Application Development"},{"content":"Database transactions are a crucial aspect in application development, especially in projects that demand high data consistency. This article will discuss how to perform database transactions in the service layer (logic), while maintaining clean architecture principles and separation of concerns.\nArchitecture Towards Database Transactions In popular architectures like Clean Architecture, Hexagonal Architecture, or Domain-Driven Design (DDD) approaches, separation of responsibilities is key. We generally divide code into several layers, for example Handler -\u0026gt; Service -\u0026gt; Repository. The service layer ideally contains pure business logic without depending on external libraries, while the repository is responsible for database interactions.\nHowever, when implementing database operations that comply with ACID (Atomicity, Consistency, Isolation, Durability) principles, a question arises: where should database transaction logic be placed? In the logic layer or in the repository layer? This often becomes a dilemma for programmers, especially due to challenges arising from architectural principles that push for breaking datastore access through various small and modularized repositories.\nnote: Atomicity means ensuring that a series of operations in one transaction must either completely succeed or completely fail.\nAs an illustration, let\u0026rsquo;s consider the case of money transfer between accounts: \u0026ldquo;Transfer money from account A to account B, update all related data, and if it fails, cancel the entire process.\u0026rdquo; There are two common approaches:\nApproach A: Transaction Logic in Repository This approach is simple because transactions are started and managed directly in the repository layer. However, this approach has weaknesses: business logic (money transfer) is mixed with data access logic. Imagine if there are additional needs, such as sending balance events to third parties as part of transaction atomicity. Should the repository have dependencies on external services too? This clearly violates separation of concerns principles. Additionally, the service layer becomes very thin, thus eliminating the benefits of unit testing at that layer.\nApproach B: Transaction Logic in Service This approach places transaction logic in the service layer, in accordance with separation of concerns principles. However, its implementation is more challenging. How can the service layer remain independent from database libraries, like GORM, while still being able to manage transactions?\nSo, where should transaction logic be placed? In the logic layer or in the repository layer? The answer is in the logic layer. This applies both when mutation processes involve interaction with multiple data sources, and when performing data collection (aggregation). The reason is that business logic determines the valid state of a set of data at a certain time. In other words, if an aggregate is not stored in a complete and valid state, then the business operations performed will be considered inconsistent with applicable business rules. This is also in line with what I\u0026rsquo;ve read in DDD books. Domain Driven Design\nChallenges and Solutions Keeping the service layer pure from third-party dependencies while managing complex database transactions is indeed difficult. However, several techniques can be applied to overcome this problem, such as using transaction abstractions in the service without having to deal directly with transaction implementation from database libraries.\nTo maintain service layer purity and still manage database transactions effectively, we will use a layered approach with several key components:\n1. DBTX interface Defines an interface that abstracts database operations, both regular operations and operations within transactions. This allows the service layer to interact with the database without depending on specific implementations. This interface will include methods like Exec, Query, QueryRow, Begin, Commit, Rollback, and others that are needed. The good news is, if you use gorm, this doesn\u0026rsquo;t need to be done because gorm has already done it (combining both methods into 1). Here I create an example using pgx.\npackage dbtx import ( \u0026#34;github.com/jackc/pgx/v5\u0026#34; \u0026#34;github.com/jackc/pgx/v5/pgconn\u0026#34; \u0026#34;github.com/jackc/pgx/v5/pgxpool\u0026#34; ) type DBTX interface { // method used by pgx for regular operations Prepare(ctx context.Context, name, sql string) (*pgconn.StatementDescription, error) Exec(ctx context.Context, sql string, arguments ...interface{}) (commandTag pgconn.CommandTag, err error) Query(ctx context.Context, sql string, args ...interface{}) (pgx.Rows, error) QueryRow(ctx context.Context, sql string, args ...interface{}) pgx.Row // method used by pgx for transaction operations Begin(ctx context.Context) (pgx.Tx, error) Commit(ctx context.Context) error Rollback(ctx context.Context) error // DBTX combines both... } 2. PGStore Provides concrete implementation of the DBTX interface for pgx library. This structure will handle selection between regular database connections or transaction connections. PGStore will check whether the context contains an active transaction (pgx.Tx). If there is, database operations will be performed using that transaction. If not, operations will be performed using the connection pool pgxpool.\nNewPGStore functions to create PGStore instances. This function accepts pgxpool pool connections and (optionally) pgx.Tx transaction objects. This will facilitate creating PGStore instances in a consistent and controlled manner.\ntype PGStore struct { NonTX *pgxpool.Pool Tx pgx.Tx } // NewPGStore return interface can execute TX and pgx.Pool func NewPGStore(pool *pgxpool.Pool, tx pgx.Tx) DBTX { var pgstore PGStore if tx != nil { pgstore.Tx = tx return \u0026amp;pgstore } pgstore.NonTX = pool return \u0026amp;pgstore } // Begin implements DBTX func (p *PGStore) Begin(ctx context.Context) (pgx.Tx, error) { if p.Tx != nil { return nil, errors.New(\u0026#34;cannot begin inside running transaction\u0026#34;) } return p.NonTX.Begin(ctx) } // Commit implements DBTX func (p *PGStore) Commit(ctx context.Context) error { if p.Tx != nil { return p.Tx.Commit(ctx) } return errors.New(\u0026#34;cannot commit: nil tx value\u0026#34;) } // Rollback implements DBTX func (p *PGStore) Rollback(ctx context.Context) error { if p.Tx != nil { return p.Tx.Rollback(ctx) } return errors.New(\u0026#34;cannot roleback: nil tx value\u0026#34;) } // Exec implements DBTX func (p *PGStore) Exec(ctx context.Context, sql string, arguments ...interface{}) (commandTag pgconn.CommandTag, err error) { if p.Tx != nil { return p.Tx.Exec(ctx, sql, arguments...) } return p.NonTX.Exec(ctx, sql, arguments...) } // Prepare implements DBTX func (p *PGStore) Prepare(ctx context.Context, name string, sql string) (*pgconn.StatementDescription, error) { if p.Tx != nil { return p.Tx.Prepare(ctx, name, sql) } return nil, errors.New(\u0026#34;cannot prefare: pool does not have prefare method\u0026#34;) } // Query implements DBTX func (p *PGStore) Query(ctx context.Context, sql string, args ...interface{}) (pgx.Rows, error) { if p.Tx != nil { return p.Tx.Query(ctx, sql, args...) } return p.NonTX.Query(ctx, sql, args...) } // QueryRow implements DBTX func (p *PGStore) QueryRow(ctx context.Context, sql string, args ...interface{}) pgx.Row { if p.Tx != nil { return p.Tx.QueryRow(ctx, sql, args...) } return p.NonTX.QueryRow(ctx, sql, args...) } 3. ExtractTx and injectTx Functions Next we create helpers that automate the use of NewPGStore. ExtractTx is used to extract database transaction connections stored in context injectTx is used for the opposite, which is injecting database transactions into context.\npackage dbtx import ( \u0026#34;github.com/jackc/pgx/v5\u0026#34; \u0026#34;github.com/jackc/pgx/v5/pgxpool\u0026#34; ) type KeyTransaction string const TXKey KeyTransaction = \u0026#34;unique-key-transaction\u0026#34; // ExtractTx extract transaction from context and transform database into dbtx.DBTX func ExtractTx(ctx context.Context, defaultPool *pgxpool.Pool) DBTX { tx, ok := ctx.Value(TXKey).(pgx.Tx) if !ok || tx == nil { return NewPGStore(defaultPool, nil) } return NewPGStore(nil, tx) } // injectTx injects transaction to context func injectTx(ctx context.Context, tx pgx.Tx) context.Context { return context.WithValue(ctx, TXKey, tx) } 4. TxManager and WithAtomic Function WithAtomic automates the use of ExtractTx and injectTx. It\u0026rsquo;s a wrapper function that will perform ROLLBACK if it fails, and will perform COMMIT database transaction if it succeeds.\nIn short, when WithAtomic is called, the context will be filled with database transaction, then the context containing database transaction will be used to run subsequent database operations, the repository will automatically use this transaction because it performs ExtractTx every time a database command is executed.\nAt the logic layer we only deal with WithAtomic.\npackage dbtx import ( \u0026#34;log/slog\u0026#34; \u0026#34;github.com/jackc/pgx/v5/pgxpool\u0026#34; ) type TxManager interface { WithAtomic(ctx context.Context, tFunc func(ctx context.Context) error) error } type txManager struct { db *pgxpool.Pool log *slog.Logger } func NewTxManager(sqlDB *pgxpool.Pool, log *slog.Logger) TxManager { return \u0026amp;txManager{ db: sqlDB, log: log, } } // ========================================================================= // TRANSACTION // WithAtomic runs function within transaction // The transaction commits when function were finished without error func (r *txManager) WithAtomic(ctx context.Context, tFunc func(ctx context.Context) error) error { // begin transaction tx, err := r.db.Begin(ctx) if err != nil { return fmt.Errorf(\u0026#34;begin transaction: %w\u0026#34;, err) } // run callback err = tFunc(injectTx(ctx, tx)) if err != nil { // if error, rollback if errRollback := tx.Rollback(ctx); errRollback != nil { r.log.Error(\u0026#34;rollback transaction\u0026#34;, slog.String(\u0026#34;error\u0026#34;, errRollback.Error())) } return err } // if no error, commit if errCommit := tx.Commit(ctx); errCommit != nil { return fmt.Errorf(\u0026#34;failed to commit transaction: %w\u0026#34;, errCommit) } return nil } 5. WithAtomic and ExtractTx Implementation Service Layer: Service layer uses TxManager.WithAtomic to wrap business logic in transactions. This ensures that all database operations in that business logic are performed atomically.\nRepository Layer: Repository layer uses ExtractTx to get the appropriate DBTX object (transaction-based or regular connection) from context. All database operations in the repository are performed through this DBTX object.\nSo the code will be something like the following.\ntype service struct { Repo AccountStorer TxManager TxManager // helper for transactions becomes additional dependency or can be combined with repo } func (s *service) TransferMoney(ctx context.Context, input model.TransferDTO) error { // shared variable to hold results inside WithAtomic if any // result := ... // Wrapping the process with database transaction txErr := s.TxManager.WithAtomic(ctx, func(ctx context.Context) error { // Getting account A accountA, err := s.Repo.GetAccountByID(ctx, input.AccountA) if err != nil { return err // Failed to get account A } // Getting account B accountB, err := s.Repo.GetAccountByID(ctx, input.AccountB) if err != nil { return err // Failed to get account B } // Checking if account A balance is sufficient if accountA.Balance \u0026lt; input.Amount { return errors.New(\u0026#34;insufficient balance\u0026#34;) // Failed due to insufficient balance } // Reducing account A balance accountA.Balance -= input.Amount if err := s.Repo.UpdateAccount(ctx, accountA); err != nil { return err // Failed to update account A balance } // Adding amount to account B balance accountB.Balance += input.Amount if err := s.Repo.UpdateAccount(ctx, accountB); err != nil { return err // Failed to update account B balance } return nil }) if txErr != nil { return txErr } return nil } // Getting account by ID func (r *repo) GetAccountByID(ctx context.Context, id uint) (model.AccountEntity, error) { dbtx := ExtractTx(ctx, r.db) // extracting context and making regular db into DBTX interface var account model.AccountModel err := dbtx.QueryRow(ctx, \u0026#34;SELECT * FROM accounts WHERE id = $1\u0026#34;, id).Scan( /* ...scan fields of account... */ ) return account, err } // Updating account func (r *repo) UpdateAccount(ctx context.Context, account model.AccountEntity) error { dbtx := ExtractTx(ctx, r.db) // extracting context and making regular db into DBTX interface _, err := dbtx.Exec(ctx, ` UPDATE accounts SET balance = $1 WHERE id = $2`, account.Balance, account.ID) return err } By implementing the above method, we successfully separate the logic layer from dependencies on third-party libraries. In the repository example I included, it can be seen that even to change ORMs, the service layer doesn\u0026rsquo;t need any changes. YEYY.\nLet\u0026rsquo;s elaborate again, what are the advantages:\nLogic layer remains pure, not contaminated by gorm packages or other drivers. Database transactions can be controlled effectively, allowing to manage transaction scope to be kept as small as possible if needed. This approach is different from implementing transactions in middleware, which can cause the entire logic process to be within one database transaction. Code readability is maintained. Unit testing remains focused on business logic only. Sample Github Repository I include sample code in two versions, one for GORM and another for other implementations (pgx). Here, GORM is simpler because basically GORM has combined regular database operations with database transaction operations.\nHere\u0026rsquo;s the repository: REPOSITORY\nWhen implementing database transactions, it\u0026rsquo;s also important to consider the possibility of deadlocks. In the sample code I provided above, I have simplified the code by setting aside those aspects. I will discuss deadlocks further in future opportunities.\n","permalink":"https://blog.muchlis.dev/en/post/db-transaction/","summary":"\u003cp\u003eDatabase transactions are a crucial aspect in application development, especially in projects that demand high data consistency. This article will discuss how to perform database transactions in the service layer (logic), while maintaining clean architecture principles and separation of concerns.\u003c/p\u003e","title":"Database Transaction Implementation Techniques in Logic Layer for Golang Backend"},{"content":"Pagination is a technique for dividing database query results into smaller chunks. Using LIMIT OFFSET queries is the most commonly used method. However, this method has several weaknesses, especially in terms of performance on very large datasets. This article will discuss problems that arise when using LIMIT OFFSET and explore more efficient alternatives, such as cursor-based pagination and seek method.\nThe Importance of Pagination and Its Challenges Pagination has several benefits, such as:\nPerformance Maintenance: Returning large data all at once is slow and resource-intensive. By dividing data into smaller chunks, APIs can return data faster and with fewer resources. Processing large data also requires a lot of memory, which can be a problem for devices with limited resources like mobile phones. By using pagination, APIs can limit the amount of data that needs to be stored in memory at any given time.\nUser Experience: For client applications that display data to users, pagination can improve user experience by providing a faster and more responsive interface. Users can see initial results quickly and can request additional data as needed.\nHowever, it\u0026rsquo;s important to remember that pagination is not always a perfect solution. On very large datasets, pagination techniques can face challenges that will become very fatal later on.\nLIMIT OFFSET Pagination Here we will discuss the disadvantages of pagination using LIMIT OFFSET and how to minimize these disadvantages.\nWhy is LIMIT OFFSET Slow for Large Datasets? When dealing with very large datasets, pagination using LIMIT OFFSET often experiences performance degradation. This is because every time we request a new page, the database must scan the entire table from the beginning to find the appropriate data, even though we only need a small portion of the data.\nHere\u0026rsquo;s an example SQL query showing how LIMIT and OFFSET are applied:\nSELECT * FROM records ORDER BY id LIMIT 10 OFFSET 1000; Explanation:\nLIMIT determines the maximum number of rows returned.\nOFFSET determines how many rows to skip before starting to return results.\nIn the example above, the query will actually scan the first 1000 rows, discard unnecessary data, and return the next 10 rows. If the table has millions of rows, skipping a large number of rows with a large offset will make the query run slower because the database must sort and scan all those rows before returning results.\nThis means if the client makes requests for page 2, page 3 and so on, it will cause the database to process many times more data compared to the amount actually returned to the client.\nAs an illustration, assuming 1 page displays 100 data:\nFor page 1: OFFSET 0, LIMIT 100 -\u0026gt; scans and returns 100 rows. For page 2: OFFSET 100, LIMIT 100 -\u0026gt; scans and discards 100 rows, then scans and returns the next 100 rows. For page 3: OFFSET 200, LIMIT 100 -\u0026gt; scans and discards 200 rows, then scans and returns the next 100 rows. For page 100: OFFSET 10000, LIMIT 100 -\u0026gt; scans and discards 10000 rows, then scans and returns the next 100 rows. The larger the offset value, the more rows need to be scanned and discarded, making the query slower and less efficient. This becomes very bad for tables with millions of rows because processing and discarding a lot of data every time there\u0026rsquo;s a new page request.\nWorst case example for this: Client wants to get all data by scanning from page 1 to the last page. Looking at the behavior, this is usually needed by other services that use our service as their data source.\nImagine we want to read a very thick book page by page. If we use the LIMIT and OFFSET method, we have to open the book from the beginning every time we want to read the next page. This is certainly very inefficient, because we will repeatedly open the same pages. In the database context, this is the same as making the database work harder than it should. Therefore, if the goal is to get all data, it\u0026rsquo;s better to take the entire book (data) at once without pagination, then read it (process it) in the application.\nImpact of COUNT(*) Query on Performance Not only that, in pagination implementation using LIMIT and OFFSET, the SELECT COUNT(*) query is often used to count the total number of rows in the dataset. This information is needed to compile pagination metadata, such as total number of pages and total items, which is then returned in the API response.\nFor example, the API response might have a structure like this:\n{ \u0026#34;message\u0026#34;: \u0026#34;successfully fetch data\u0026#34;, \u0026#34;data\u0026#34;: [ {} ], \u0026#34;meta\u0026#34;: { \u0026#34;current_page\u0026#34;: 1, \u0026#34;page_size\u0026#34;: 100, \u0026#34;total_count\u0026#34;: 3000, \u0026#34;total_page\u0026#34;: 30 }, \u0026#34;trace_id\u0026#34;: \u0026#34;5b427ba9ab30002d347ea17cf8000cca\u0026#34; } To generate this metadata, the backend needs to perform two queries:\nTo retrieve data with LIMIT and OFFSET SELECT * FROM users LIMIT 100 OFFSET 0; To count the total number of rows with COUNT(*) SELECT COUNT(*) FROM users; Surprisingly, using COUNT(*) on large datasets can result in significant performance degradation. This is due to:\nFull table scan: The database needs to scan the entire table to count the number of rows, especially if there are no suitable indexes. Lack of index optimization: COUNT(*) often cannot be optimized with indexes, so query execution time becomes longer. Concurrency and locking issues: COUNT(*) queries can cause locks with other queries and hinder system performance. High I/O load: The process of counting rows requires many read-write operations on the database disk. This problem may not be clearly visible early in development, but will become more apparent as data volume continues to grow. Therefore, I highly recommend that we can determine the most suitable pagination technique from the beginning of development. Alternative techniques and optimizations can be good solutions to overcome them.\nLIMIT OFFSET Database Query Optimization It turns out that queries for pagination with LIMIT OFFSET can still be optimized. How to do it? I actually found this technique in a library used in another language, PHP Laravel. which can be found in this library: https://github.com/hammerstonedev/fast-paginate What is done to make the performance better?\nselect * from users -- The full data that you want to show your users. where users.id in ( -- The \u0026#34;deferred join\u0026#34; or subquery, in our case. select id from users -- The pagination, accessing as little data as possible - ID only. limit 15 offset 150000 ) The idea is to apply LIMIT and OFFSET on data with a smaller scope, then search for the results to create complete data.\nHowever, the SELECT COUNT(*) query may not be optimizable. So, this optimization technique on LIMIT OFFSET queries doesn\u0026rsquo;t completely solve the problems I experienced, especially for SELECT COUNT(*) queries on large datasets. This is evident from the monitoring results I conducted.\nThe monitoring results show a significant performance difference between query to retrieve data vs COUNT(*) query, especially when many requests come in simultaneously.\nThis is also supported by similar problems discussed on the internet such as:\nhttps://stackoverflow.com/questions/55018986/postgresql-select-count-query-takes-long-time https://www.reddit.com/r/PostgreSQL/comments/140b4xy/select_count_is_slow_in_large_tables/ https://tunghatbh.medium.com/postgresql-slow-count-query-c93c30792606 From this case study, I draw several important conclusions:\nNumber of N queries doesn't always determine performance: It\u0026rsquo;s not always true that the fewer query requests we run, the better the performance. In some cases, splitting complex queries into several smaller queries can actually improve overall performance. Indexes are not always optimal for COUNT(*): Although indexes can improve query performance in general, in COUNT(*) cases indexes are not always effective. Importance of benchmarking: Comparing performance before and after query changes is the most accurate way to measure the impact of an optimization. Because different queries and data structures may require different optimization methods. Cursor! As an Alternative to Limit Offset Cursor-based Pagination Explanation Cursor-based pagination uses a unique value from a column (usually the sorted column) as a \u0026ldquo;cursor\u0026rdquo; to mark the current position in the query results. Instead of using offset, we send the cursor from the previous result to get the next page. This is more efficient because the database can skip values and only needs to look for records that have cursor values greater than the previous cursor value.\nSELECT * FROM users WHERE sort_column \u0026gt; \u0026#39;cursor_value\u0026#39; ORDER BY sort_column LIMIT 10; Advantages of Cursor-based Pagination Better performance: No need to scan the entire table for each page request. Consistent results: Query results are always the same, regardless of data changes that occur between requests. For example, pagination on LIMIT OFFSET will be inconsistent if data on previous pages is deleted. Infinity Loading UX: Cursor pagination is very suitable for web and mobile user experience that usually implements infinity loading. Disadvantages of Cursor-based Pagination More complex implementation: Requires careful planning in choosing the right cursor column. Not suitable for all types of queries and UX: Only effective for queries sorted by one or several columns. Stateful: Because it must pass the cursor Sorting merged with cursor: The order of displayed data is always proportional to the cursor used. Implementation Example As an example, API response with cursor pagination might have a structure like this:\nEndpoint: {baseURL}/users?limit=3\u0026amp;cursor= Query Param: limit: amount of data displayed. cursor: cursor input, for the first page fill in blank. cursor_type: what field is used as cursor, usually has a default value, in this example using ulid descending. { \u0026#34;message\u0026#34;: \u0026#34;successfully fetch data\u0026#34;, \u0026#34;data\u0026#34;: [ { \u0026#34;ulid\u0026#34;: \u0026#34;01J4EXF94RZA4AZG1C0A0C2RKF\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;muchlis\u0026#34; }, { \u0026#34;ulid\u0026#34;: \u0026#34;01J4EXF94RWZVWS9NVEZMQ3R1N\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;monkey d luffy\u0026#34; }, { \u0026#34;ulid\u0026#34;: \u0026#34;01J4EXF94RT7G5CRH047MC0EF1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;portgas d ace\u0026#34; } ], \u0026#34;meta\u0026#34;: { \u0026#34;current_cursor\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;next_cursor\u0026#34;: \u0026#34;01J4EXF94RT7G5CRH047MC0EF1\u0026#34;, \u0026#34;next_page\u0026#34;: \u0026#34;/users?limit=3\u0026amp;cursor=01J4EXF94RT7G5CRH047MC0EF1\u0026#34;, \u0026#34;prev_cursor\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prev_page\u0026#34;: \u0026#34;/users?limit=3\u0026amp;cursor=\u0026#34; }, \u0026#34;trace_id\u0026#34;: \u0026#34;5b427ba9ab30002d347ea17cf8000cca\u0026#34; } Repo Layer:\nI use raw queries for easier readability. But in reality I usually use sql builders like golang squirrel or goqu.\nfunc (r *repo) FetchUserByUlid(ctx context.Context, cursorID string, limit uint64) ([]entity.User, error) { ctx, cancel := context.WithTimeout(ctx, 2*time.Second) defer cancel() var sqlStatement string var args []interface{} if cursorID != \u0026#34;\u0026#34; { sqlStatement = ` SELECT id, name FROM users WHERE id \u0026lt; $1 ORDER BY id DESC LIMIT $2; ` args = append(args, cursorID, limit) } else { sqlStatement = ` SELECT id, name FROM users ORDER BY id DESC LIMIT $1; ` args = append(args, limit) } // Execute the query rows, err := r.db.Query(ctx, sqlStatement, args...) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to execute query: %w\u0026#34;, err) } defer rows.Close() users := make([]entity.User, 0) // [SKIP] Parse the results // [SKIP] Check for errors after iterating over rows return users, nil } Service Layer:\nAt the service layer, there is logic where we have to get more data than what will be displayed to know whether the next data exists or not.\nfunc (s *Service) FetchAllUsersWithCursor(ctx context.Context, cursor string, limit uint64) ([]entity.User, *string /*next cursor*/, error) { // [SKIP] validation, tracer etc // Call repo layer to retrieve data // Add Limit +1 so we know if there is continuation data or not // This excess data will be discarded later results, err := s.repo.FetchUserByUlid(ctx, cursor, limit+1) if err != nil { return nil, nil, fmt.Errorf(\u0026#34;error FetchUserByUlid: %w\u0026#34;, err) } // Determine next cursor var nextCursor *string if len(results) \u0026gt; int(limit) { nextCursorID := results[limit-1].ID // Set cursor if more data than limit is found nextCursor = \u0026amp;nextCursorID results = results[:limit] // Remove excess data } else { nextCursor = nil // If there is no excess data, the next cursor is set to nil } // [SKIP] Convert results return results, nextCursor, nil } Assuming using Clean Architecture or Hexagonal Architecture\n+-------------------------------------+ | HTTP Handler | | (Handling HTTP requests and | | responses, routing, etc.) | +----------------+--------------------+ | v +----------------+--------------------+ | Service | | (Business logic, orchestrating the | | application flow, validation, | | calling Repositories) | +----------------+--------------------+ | v +----------------+--------------------+ | Repository | | (Data access layer, interacting with| | the database, etc.) | +-------------------------------------+ In the code example above, the remaining layer is the HTTP Handler which serves as the View, where that layer is responsible for creating other values from the service layer process results such as temporarily storing current_cursor, creating next_page values from the FetchAllUsersWithCursor() return value and various other values for responses that require the HTTP Handler Framework.\nThat was an example of a simplified cursor pagination implementation so we get a little picture of its complexity. Even in the code above I intentionally skipped several things below because they are optional.\nPrevious_Page requires implementation that is the opposite of the default SQL Query. Instead of using cursor and order default WHERE id \u0026lt; $1 ORDER BY id DESC, it becomes WHERE id \u0026gt; $1 ORDER BY id ASC with cursor being the first value of the data displayed on the current page. The possibility of cursors and orders that require 2 keys or even more. More strict value validation for Cursor and OrderBy types. Benchmark: Analysis:\nLimit-Offset Pagination has response times that start to increase significantly after page 50 (meaning at data depth 50,000), showing poor scaling for large datasets. Limit-Offset + Query Count Pagination (run together with goroutine) Slightly slower than regular Limit-Offset, which shows additional overhead. The additional overhead from this count will be felt when requests are run in parallel. While the test above was done sequentially. Cursor Pagination is the most efficient and stable, suitable for large datasets with many pages. Notes:\nThis comparison compares all methods using uniform specifications and conditions. Network latency, structure and amount of data can affect results. So, just rely on the comparison, because the numbers will vary greatly depending on each condition. The data used includes 100,000 user data entries that are left-joined with 2 small tables with extreme pagination trials of 1,000 entries per page. Testing was done sequentially, page by page. This test does not include other factors that are actually important such as memory usage, CPU, rows computed in the database. Here I try to make minimal effort. Theoretically alone we can actually estimate which method is superior. This benchmark confirms that. Limit-Offset Pagination has the advantage of ease of implementation, with weaknesses that will only be felt when our application reaches a level where the amount of data becomes very large. This weakness can also be overcome by providing indexed range and filtering predetermined data to users (such as when displaying bank transaction data that must determine the month).\nOn the other hand, although faster and more stable, Cursor-Based Pagination is slightly more complicated to implement and has certain limitations that may make it less suitable for all types of cases.\nThe saying premature optimization is the root of all evil reminds us that optimization that is too early can be a problem, but here in my personal opinion, avoiding mistakes from the start doesn\u0026rsquo;t mean it\u0026rsquo;s a bad thing either. In fact, making optimal architectural and design decisions, such as choosing Cursor-Based Pagination over Limit-Offset, can be considered a wise decision-making, not just premature optimization. Essentially, understanding the trade-offs of each choice and choosing the right solution for specific needs is a more appropriate approach in software development.\n","permalink":"https://blog.muchlis.dev/en/post/pagination/","summary":"\u003cp\u003ePagination is a technique for dividing database query results into smaller chunks. Using LIMIT OFFSET queries is the most commonly used method. However, this method has several weaknesses, especially in terms of performance on very large datasets. This article will discuss problems that arise when using LIMIT OFFSET and explore more efficient alternatives, such as cursor-based pagination and seek method.\u003c/p\u003e","title":"Pagination Optimization: Why Limit-Offset Can Be a Time Bomb and Cursor Pagination as the Solution"},{"content":"Often, the Golang programs we create are not just REST-API servers, but also include other functions such as Event Consumers, Schedulers, CLI Programs, Database Backfills, or combinations of all of them. This project structure guideline can be used to enable all of that. This structure focuses on separating core logic from external dependencies, allowing code reuse across various application modes.\nRepository Link: https://github.com/muchlist/templaterepo\nPrinciples and Goals: Development Consistency: Providing uniform methods in building applications to improve team understanding and collaboration. Modularity: Ensuring code separation between modules and avoiding tight coupling, making maintenance and further development easier. Effective Dependency Management: Avoiding circular dependency errors despite having many interconnected modules, through applying dependency inversion principles. Testable Code: Applying Hexagonal architecture principles to separate core logic from external dependencies, thereby improving flexibility and ease of testing. Conceptual Hexagonal Architecture Hexagonal architecture, also known as ports and adapters architecture, focuses on separating core logic from external dependencies. This approach supports the design principles mentioned above by ensuring that the application core remains clean and isolated from external components.\nCore: Contains the application\u0026rsquo;s business logic. Ports: A collection of abstractions that define how external parts of the system can interact with the core. Ports can be interfaces used by the core to interact with external components such as databases, notification providers, etc. I usually use Golang idioms in naming these interface types like storer, reader, saver, assumer. Adapters: Implementations of ports. Adapters implement the interfaces defined by ports to connect the core with external components. Project Structure ├── app │ ├── api-user │ │ ├── main.go │ │ └── url_map.go │ ├── consumer-user │ │ └── main.go │ └── tool-logfmt │ └── main.go ├── business │ ├── complex │ │ ├── handler │ │ │ └── handler.go │ │ ├── helper │ │ │ └── formula.go │ │ ├── port │ │ │ └── storer.go │ │ ├── repo │ │ │ └── repo.go │ │ └── service │ │ └── service.go │ ├── notifserv │ │ └── service.go │ └── user │ ├── handler.go │ ├── repo.go │ ├── service.go │ ├── storer.go │ └── worker.go ├── conf │ ├── conf.go │ └── confs.go ├── go.mod ├── go.sum ├── migrations │ ├── 000001_create_user.down.sql │ └── 000001_create_user.up.sql ├── models │ ├── notif │ │ └── notif.go │ └── user │ ├── user_dto.go │ └── user_entity.go └── pkg ├── db-pg │ └── db.go ├── errr │ └── custom_err.go ├── mid │ └── middleware.go ├── mlog │ ├── log.go │ └── logger.go └── validate └── validate.go Folder: app/ The App folder stores code that cannot be reused. The focus of code in this folder includes:\nThe starting point of the program when executed (starting and stopping the application). Assembling dependency code required by the program. Specific to input/output operations. In most other projects, this folder would be named cmd. It\u0026rsquo;s named app because the folder position will be at the top (which feels quite good) and adequately represents the folder\u0026rsquo;s function.\nInstead of using frameworks like Cobra to choose which application to run, we use the simplest method such as running the program with go run ./app/api-user for the API-USER application and go run ./app/consumer-user for the KAFKA-USER-CONSUMER application.\nFolder: pkg/ Contains packages that can be reused anywhere, usually basic elements not related to business modules, such as logger, web framework, or helpers. A place to put libraries that have been wrapped to make them easy to mock. Both application layer and business layer can import this pkg.\nUsing pkg/ as a container for code that you initially weren\u0026rsquo;t sure where to place has proven to speed up the development process. Questions like \u0026quot;Where should I put this?\u0026quot; will get the default answer \u0026quot;Put it in pkg/.\u0026quot;.\nFolder: business/ or internal/ Contains code related to business logic, business problems, business data.\nFolder: business/{domain-name}/* In each business domain, there\u0026rsquo;s a service layer (or core in hexagonal terms) that must remain clean from external libraries. This includes layers for accessing persistent data (repo) and interfaces that function as ports.\nFolder: business/{domain-name}/{subfolder} Sometimes, a domain can become very complex, requiring separation of service, repo, and other elements into several parts. In such cases, we prefer to organize and separate these components into different folders, which will also require using different packages. For example, business/complex.\nFolder: models Models (including DTOs, Payloads, Entities) are usually placed within their respective business packages. However, in complex cases where application A needs models B and C, we can consider placing these models at a higher level so they can be accessed by all parts that need them.\nSeparating structs between Entity, DTO, and Model is quite important to maintain flexibility and code cleanliness. This is because:\nWhat is consumed by business logic will not always be exactly the same as the database model. The response received by users will not always be exactly the same as the database table. And so on. Read: Understanding the Importance of Separating DTO, Entity and Model in Application Development\nRules It\u0026rsquo;s very important to create and update agreed-upon rules so that all parties follow a consistent approach. For example, this repository template is based on its ability to avoid tightly-coupled code, so the Code Dependency Writing Rules become very important to follow.\nThese rules will grow over time. For example, what often causes disagreement:\nHow deep should if-else conditions be allowed How to perform database transactions in the logic layer?. And so on. Also read Database Transaction Implementation Techniques in Logic Layer for Golang Backend\nCode Dependency Writing Rules Using Dependency Injection: Dependency Injection (DI) is a design pattern where dependencies are provided from outside the object. This helps manage dependencies between components, makes code more modular, and facilitates testing. So, modules that depend on each other must depend on abstractions.\nExample constructor for creating user service logic business/user/service.go\ntype UserService struct { storer UserStorer notifier NotifSender } // NewUserService requires UserStorer and NotifSender. // UserStorer and NotifSender are abstractions required by UserService // Objects that will fulfill UserStorer and NotifSender will be determined by // dependency configuration in the /app folder. // UserStorer and NotifSender can also be mocked for easy testing func NewUserService(store UserStorer, notifier NotifSender) *UserService { return \u0026amp;UserService{storer: store, notifier: notifier} } Applying Dependency Inversion Principle: In the business layer, especially for the logic part (usually named service.go or usecase.go or core), communication between layers relies on abstractions and strong application of the dependency inversion principle. In Golang, true dependency inversion can be achieved as explained in the following diagram.\nRegarding interface positioning, it\u0026rsquo;s best to place them in the module that needs them. This has been discussed in the book 100 Go Mistakes and How to Avoid Them and several other books.\nFor example, the business/user domain needs a function to send notifications that can be fulfilled by business/notifserv, but business/user doesn\u0026rsquo;t explicitly say it needs business/notifserv, but rather says \u0026quot;I need a unit that can execute SendNotification()\u0026quot; \u0026ndash; period. The dependency implementation can be seen in app/api-user/routing.go. This method prevents circular dependency import errors and ensures code remains loosely coupled between domains.\nExample dependencies needed to create user core logic business/user/storer.go:\npackage user import ( \u0026#34;context\u0026#34; modelUser \u0026#34;templaterepo/models/user\u0026#34; ) // UserStorer is an interface that defines operations that can be performed on the user database. // This interface belongs to the service layer and is intended to be written in the service layer part // Although we know exactly that the implementation is in business/user/repo.go, the service layer (core) still only depends on this interface. // The concrete implementation of this interface will be determined by dependency configuration in the /app folder. type UserStorer interface { Get(ctx context.Context, uid string) (modelUser.UserDTO, error) CreateOne(ctx context.Context, user *modelUser.UserEntity) error } // NotifSender is an interface that defines operations for sending notifications. // This interface belongs to the service layer and is intended to be written in the service layer part // The object used to send notifications will be determined by dependency configuration in the /app folder. type NotifSender interface { SendNotification(message string) error } Example constructor for creating notification business/notifserv/service.go\npackage notifserv type NotifService struct{} // return concrete struct, not its interface // because NotifService is not constrained to only be NotifSender func NewNotifServ() *NotifService { return \u0026amp;NotifService{} } // SendNotification is required to fulfill the NotifSender interface in user service func (n *NotifService) SendNotification(message string) error { // TODO : send notif to other server return nil } // SendWhatsapp is not required by user service but might be needed by other services func (n *NotifService) SendWhatsapp(message string, phone string) error { // TODO : send whatsapp return nil } Other Agreed Rules Follow Uber\u0026rsquo;s style guide as a base (https://github.com/uber-go/guide/blob/master/style.md). This rule will be overridden if there are rules written here. Configuration files should only be accessed in main.go. Other layers that want to access configuration must receive it through function parameters. Configuration must have default values that work in local environment, which can be overridden by .env files and command line arguments. Errors must be handled only once and must not be ignored. This means either consume or return, but not both simultaneously. Example consumption: writing error to log, example return: returning error if error is not nil. Don\u0026rsquo;t expose variables in packages. Use combination of private variables and public functions instead. When code is widely used, create helper.go. But if used in several packages, create a new package (for example to extract errors that only exist in user, /business/user/ipkg/error_parser.go). If usage is very broad, put it in /pkg (for example, pkg/slicer/slicer.go, pkg/datastructure/ds.go, pkg/errr/custom_error.go). Follow Golang idioms. Name interfaces with -er or -tor suffixes to indicate they are interfaces, such as Writer, Reader, Assumer, Saver, Reader, Generator. (https://go.dev/doc/effective_go#interface-names). Example: In a project with three layers: UserServiceAssumer, UserStorer, UserSaver, UserLoader. Tools Makefile Makefile contains commands to help run applications quickly because you don\u0026rsquo;t have to remember all the long commands. Functions like aliases. The way is to write commands in the Makefile like the following example.\nThe top line is a comment that will appear when calling the helper. .PHONY is a marker so the terminal doesn\u0026rsquo;t consider makefile commands as file access. run/tidy: is an alias for the commands inside it.\n## run/tidy: run golang formatter and tidying code .PHONY: run/tidy run/tidy: @echo \u0026#39;Tidying and verifying module dependencies...\u0026#39; go mod tidy go mod verify @echo \u0026#39;Formatting code...\u0026#39; go fmt ./... As an example, to run the applications in this repository we can use commands like below:\n# 1. ensure availability of dependencies like database etc. # 2. run application with makefile (see Makefile) $ make run/api/user # that command will execute $ go run ./app/api-user # so the http server mode of the application will be run pre-commit It\u0026rsquo;s recommended to use pre-commit (https://pre-commit.com/).\n// init pre-commit install // precommit will be triggered every commit // manual pre-commit run --all-files ","permalink":"https://blog.muchlis.dev/en/post/structuring-project-folder/","summary":"\u003cp\u003eOften, the Golang programs we create are not just REST-API servers, but also include other functions such as Event Consumers, Schedulers, CLI Programs, Database Backfills, or combinations of all of them. This project structure guideline can be used to enable all of that. This structure focuses on separating core logic from external dependencies, allowing code reuse across various application modes.\u003c/p\u003e\n\u003cp\u003eRepository Link: \u003ca href=\"https://github.com/muchlist/templaterepo\"\u003ehttps://github.com/muchlist/templaterepo\u003c/a\u003e\u003c/p\u003e","title":"Folder Structure and Code Writing Rules in Golang Projects: Personal Preferences"},{"content":"In backend development with Golang, managing background processes using goroutines is a common practice that can improve application performance. However, there are several common problems often encountered when implementing goroutines, especially regarding panic handling, context management, and proper shutdown processes. This article will review some common mistakes related to goroutine usage and how to overcome them.\nCommon Problems in Goroutine Usage Panics inside sub goroutines are not included in the main goroutine\u0026rsquo;s recovery area. Context passed to goroutines can be subject to deadline or cancellation when the main goroutine finishes execution. Graceful shutdown can still ignore background processes that are being processed. 1. Handling Panic Inside Sub Goroutines Many developers assume that panics in all code in HTTP services will be recovered by recovery middleware. However, panic recovery only applies to one goroutine. If we call another goroutine, we need additional recovery code. Here\u0026rsquo;s an example:\nfunc main() { // panic recovery for main program defer func() { if err := recover(); err != nil { fmt.Printf(\u0026#34;panic recovered: %s\u0026#34;, err) } }() go func() { // panic recovery for sub goroutine defer func() { if err := recover(); err != nil { fmt.Printf(\u0026#34;panic recovered: %s\u0026#34;, err) } }() // Running in background publish(context.Background(), response) }() ... } To make it easier, we can create a helper function as follows:\nfunc Background(fn func()) { go func() { defer func() { if err := recover(); err != nil { fmt.Printf(\u0026#34;panic recovered: %s\u0026#34;, err) } }() fn() }() } Using this helper function, the previous example code can be changed to:\nfunc main() { // panic recovery for main program defer func() { if err := recover(); err != nil { fmt.Printf(\u0026#34;panic recovered: %s\u0026#34;, err) } }() Background(func() { publish(context.Background(), response) }) ... } 2. Managing Context in Goroutines Context is always used in Golang programs to pass important data such as tracing identification, request_id, and for process canceling needs. However, context passed to goroutines can cause problems, especially if the context finishes faster than the goroutine. For example, context from HTTP requests is passed to functions running in different goroutines. If that context finishes, then the process in the goroutine will be canceled if it\u0026rsquo;s aware of context cancellation.\nExample:\nfunc SampleHandler(w http.ResponseWriter, r *http.Request) { response, err := doSomeTask(r.Context(), r) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } go func() { // suppose publish takes 2 seconds // and is aware of context status err := publish(r.Context(), response) }() // SampleHandler finishes in 1 second ... } With the above example, publish will fail and get a context canceled error. To overcome this, we can replace r.Context() with context.Background(). However, what if we need values inside the context? The solution is to create our own context implementation:\ntype Detach struct { ctx context.Context } func (d Detach) Deadline() (time.Time, bool) { return time.Time{}, false } // done signal will be ignored func (d Detach) Done() \u0026lt;-chan struct{} { return nil } func (d Detach) Err() error { return nil } func (d Detach) Value(key any) any { return d.ctx.Value(key) } Using this custom context, cancellation signals from the parent context will have no effect, while other values remain the same. Here\u0026rsquo;s its application to the previous example:\nfunc SampleHandler(w http.ResponseWriter, r *http.Request) { response, err := doSomeTask(r.Context(), r) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } go func() { // suppose publish takes 2 seconds // publish will continue even if it takes longer than main func err := publish(Detach{ctx: r.Context()}, response) }() // SampleHandler finishes in 1 second ... } 3. Performing Graceful Shutdown with Goroutines Graceful shutdown is the process of waiting for all processes to finish before the application is completely stopped. In HTTP servers, the steps are usually as follows:\nGet application terminate signal. Close HTTP server so no requests come in. Wait for all processes in one request-response cycle to finish. Close all database connections. However, what about processes still running in goroutines? If the process is important (for example invalidate cache), we can use sync.WaitGroup to detect if there are still unfinished processes. Here\u0026rsquo;s example code using sync.WaitGroup:\nimport ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) // wgProcess waitgroup for gracefully shutdown background process var wgProcess sync.WaitGroup func Background(fn func()) { wgProcess.Add(1) go func() { defer wgProcess.Done() defer func() { if err := recover(); err != nil { log.Error(fmt.Sprintf(\u0026#34;panic when run background process\u0026#34;), fmt.Errorf(\u0026#34;%s\u0026#34;, err)) } }() fn() }() } This code ensures that all Background processes are recorded for start and completion through waitgroup. In the main program that implements Graceful Shutdown, we add wgProcess.Wait() so the process blocks until the waitgroup is 0 (when all processes finish running). Make sure that functions adding sync.WaitGroup can stop, or add timeout.\nBy understanding and implementing the solutions above, you can manage background processes more effectively in Golang. Always make sure to handle panic in every goroutine, manage context properly, and perform application shutdown properly so all processes can finish correctly.\n","permalink":"https://blog.muchlis.dev/en/post/safe-goroutine/","summary":"\u003cp\u003eIn backend development with Golang, managing background processes using goroutines is a common practice that can improve application performance. However, there are several common problems often encountered when implementing goroutines, especially regarding panic handling, context management, and proper shutdown processes. This article will review some common mistakes related to goroutine usage and how to overcome them.\u003c/p\u003e","title":"Pay Attention to These Things When Using Golang Goroutines"},{"content":"Profiling is the process of measuring application performance to identify and analyze various aspects that affect performance, such as CPU usage, memory, and goroutines. Profiling is very important in the development process to ensure applications run efficiently and optimally and to detect anomalies.\nProfiling Goals in this article Detect memory leaks. Identify slow-running code. Optimize code. Example of profiling output in Golang:\nPreparation Code Modification\nTo be able to perform profiling, you need to import the net/http/pprof package so our service can run and expose the /debug/pprof endpoint. However, instead of using the main HTTP server, it\u0026rsquo;s better if the debug endpoint is exposed separately so there\u0026rsquo;s no unintended data leakage.\nThe implementation is like the example below:\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;net/http/pprof\u0026#34; ) func debugMux() *http.ServeMux { mux := http.NewServeMux() // Register all the standard library debug endpoints. mux.HandleFunc(\u0026#34;/debug/pprof/\u0026#34;, pprof.Index) mux.HandleFunc(\u0026#34;/debug/pprof/cmdline\u0026#34;, pprof.Cmdline) mux.HandleFunc(\u0026#34;/debug/pprof/profile\u0026#34;, pprof.Profile) mux.HandleFunc(\u0026#34;/debug/pprof/symbol\u0026#34;, pprof.Symbol) mux.HandleFunc(\u0026#34;/debug/pprof/trace\u0026#34;, pprof.Trace) return mux } func main() { config := cfg.Load() ctx := context.Background() debugPort := 4000 serverPort := 8080 // start debug server in other goroutine using port 4000 debugMux := debugMux() go func(mux *http.ServeMux) { if err := http.ListenAndServe(fmt.Sprintf(\u0026#34;0.0.0.0:%v\u0026#34;, debugPort), mux); err != nil { log.Error(\u0026#34;serve debug api\u0026#34;, err) } }(debugMux) // start main server in main goroutine using port 8080 webApi := web.New(app.logger, serverPort, config.App.Env, config.App.Name) err = webApi.Serve(app.routes()) if err != nil { log.Error(\u0026#34;serve web api\u0026#34;, err) } } In the above example, we run two HTTP servers: port 4000 for debug/profiling and 8080 for the main program.\nTesting Debug Endpoint.\nWhen the server is running, hitting the endpoint http://localhost:4000/debug/pprof/ will display a web page like the following:\nOn this page, we can see what benefits and what data we can analyze from this endpoint.\nGenerally used are:\nallocs: to analyze memory based on samples heap: to analyze memory in running programs profile: to analyze CPU usage. Tool Requirements.\nTo analyze, we use pprof which can be run with the command go tool pprof \u0026lt;file/url\u0026gt;\nAdditional tools are Graphviz (for creating graphs)\n# ubuntu apt-get install graphviz gv # mac brew install graphviz How to Perform Memory Profiling Get Sample Heap/Allocs Data. The command below will generate a file named heap.out:\ncurl -s -v http://localhost:4000/debug/pprof/heap \u0026gt; heap.out Start Analyzing the File with pprof\ngo tool pprof heap.out Commonly used commands:\ntop: to display top memory usage data. top50: to display top results according to number (Top n). top -cum: to display top data ordered by cumulative memory. png: to display profiling data visualization as png format image. web: to display visualization through browser list : to analyze function names in more detail. Hint:\nflat shows the amount of memory or CPU time spent by that function directly, not by functions called by it. cum (cumulative) shows the total amount of memory or CPU time spent by that function and all functions called by it (recursively). Generally all memory usage can be seen with the png or web command which will display a graph like the following. The image below shows fairly normal usage. If a memory leak occurs we can easily see large boxes that are very conspicuous and will continue to grow over time:\nFor more detail, use pprof using terminal:\nUsing the top20 -cum command will display which functions use memory cumulatively (summed with functions on the stack below). We can ignore reasonable usage amounts. For example, go-chi very reasonably accumulates 19MB of memory because load testing was just performed on this service.\nFor example, suppose jack/chunkreader is suspicious. Then the next step we can run the command list github.com/jackc/chunkreader/v2.* (list command uses regex pattern)\nso it displays:\nFrom there we can see which functions are considered less optimal if the numbers don\u0026rsquo;t match.\nHow to Perform CPU Profiling Slightly different from memory profiling, CPU testing must be triggered and loaded while the sample data collection is active.\nThe following command will activate CPU profiling collection for 5 seconds. (although during testing it\u0026rsquo;s still collected for 30s)\ngo tool pprof http://localhost:4000/debug/pprof/profile\\\\?second\\\\=5 At the same time, perform load testing. You can use hey, jmeter or other load testing tools.\nThe result will be like the following:\nIn the data above, I checked custom middleware which turns out the slow process is in next.ServeHTTP, which is reasonable because of cumulative calculation (below that function the actual program will run, namely going to handler → service → repo).\nSample image when executing png command:\nGarbage Collector Performance analysis can also be seen from the number of Garbage Collector (GC) Cycles that run and also memory allocation after and before GC. Many GC Cycles running can be a sign of non-optimal memory allocation usage, although not always. Here\u0026rsquo;s how:\nRun the program with the following command:\n# Build our program first go build ./app/api # Command to run the program but only display gc logs GODEBUG=gctrace=1 ./api \u0026gt; /dev/null The log printed on the terminal is like this:\ngc 1 @0.005s 3%: 0.007+1.6+0.028 ms clock, 0.063+0.12/1.2/0.25+0.22 ms cpu, 3-\u0026gt;4-\u0026gt;1 MB, 4 MB goal, 0 MB stacks, 0 MB globals, 8 P gc 2 @0.010s 3%: 0.024+0.96+0.002 ms clock, 0.19+0/1.2/0.34+0.022 ms cpu, 3-\u0026gt;3-\u0026gt;2 MB, 4 MB goal, 0 MB stacks, 0 MB globals, 8 P gc 3 @0.014s 3%: 0.087+1.4+0.005 ms clock, 0.70+0/1.0/1.8+0.044 ms cpu, 5-\u0026gt;5-\u0026gt;5 MB, 5 MB goal, 0 MB stacks, 0 MB globals, 8 P gc 4 @0.061s 1%: 0.090+1.0+0.019 ms clock, 0.72+0.082/1.4/0+0.15 ms cpu, 11-\u0026gt;11-\u0026gt;10 MB, 12 MB goal, 0 MB stacks, 0 MB globals, 8 P How to Read logs:\ngc 4 means during the process lifetime, GC has run 4 times. 11-\u0026gt;11-\u0026gt;10 shows heap size before GC, after GC, and heap size still alive after GC in MB (Megabytes). 0.090+1.0+0.019 ms clock shows time spent in milliseconds (ms) for three main GC phases: 0.090 ms for mark. 1.0 ms for sweep. 0.019 ms for stop-the-world (STW) time. 0.72+0.082/1.4/0+0.15 ms cpu shows CPU usage in milliseconds (ms) during GC phases. 3-\u0026gt;4-\u0026gt;1 MB shows heap size before GC, after GC, and heap size still alive after GC in MB. 4 MB goal is the target heap size. 0 MB stacks, 0 MB globals show memory used by stacks and global variables. 8 P shows the number of processors (goroutine scheduler threads) used. GC performance analysis:\nWhen the program runs, test using hey or similar tool, for example with 10,000 requests and see how many GC are generated. Record request per second for comparison Run profiling like before. go tool pprof http://localhost:4000/debug/pprof/alloc # find which uses the most memory top 40 -cum list \u0026lt;name_func\u0026gt; Heap analysis:\nSee if heap remains small or grows, if it grows then there\u0026rsquo;s likely a memory leak. After making changes (if any) test again from step 1 and compare the number of GC Cycles. Performance Comparison:\nEnsure memory usage is efficient by looking at the number of GC cycles that occur, heap allocation before and after GC cycles, as well as GC time and stop-the-world (STW) time. The goal is performance improvement that can be proven by comparison with previous code. This can be done by comparing request per second. How Do We Know Our Changed Code Becomes Better? Perform profiling like above and compare the results.\nUse tools like hey for load testing and compare the output, for example request per second. Record results before and after changes.\nLook at Garbage Collector performance when load testing is performed.\nThis article outlines important steps for profiling in Golang, from preparation, code modification, to analyzing profiling results to optimize application performance.\n","permalink":"https://blog.muchlis.dev/en/post/profiling/","summary":"\u003cp\u003eProfiling is the process of measuring application performance to identify and analyze various aspects that affect performance, such as CPU usage, memory, and goroutines. Profiling is very important in the development process to ensure applications run efficiently and optimally and to detect anomalies.\u003c/p\u003e","title":"Profiling Techniques in Golang"}]